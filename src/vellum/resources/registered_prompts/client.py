# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

import pydantic

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ...errors.conflict_error import ConflictError
from ...types.provider_enum import ProviderEnum
from ...types.register_prompt_error_response import RegisterPromptErrorResponse
from ...types.register_prompt_model_parameters_request import RegisterPromptModelParametersRequest
from ...types.register_prompt_prompt_info_request import RegisterPromptPromptInfoRequest
from ...types.register_prompt_response import RegisterPromptResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class RegisteredPromptsClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def register_prompt(
        self,
        *,
        label: str,
        name: str,
        prompt: RegisterPromptPromptInfoRequest,
        provider: ProviderEnum,
        model: str,
        parameters: RegisterPromptModelParametersRequest,
        meta: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
    ) -> RegisterPromptResponse:
        """
        <strong style="background-color:#ffc107; color:white; padding:4px; border-radius:4px">Unstable</strong>

        Registers a prompt within Vellum and creates associated Vellum entities. Intended to be used by integration
        partners, not directly by Vellum users.

        Under the hood, this endpoint creates a new sandbox, a new model version, and a new deployment.

        Parameters:
            - label: str. A human-friendly label for corresponding entities created in Vellum. <span style="white-space: nowrap">`non-empty`</span>

            - name: str. A uniquely-identifying name for corresponding entities created in Vellum. <span style="white-space: nowrap">`non-empty`</span>

            - prompt: RegisterPromptPromptInfoRequest. Information about how to execute the prompt template.

            - provider: ProviderEnum. The initial LLM provider to use for this prompt

                                      * `ANTHROPIC` - Anthropic
                                      * `COHERE` - Cohere
                                      * `GOOGLE` - Google
                                      * `HOSTED` - Hosted
                                      * `MOSAICML` - MosaicML
                                      * `MYSTIC` - Mystic
                                      * `OPENAI` - OpenAI
                                      * `PYQ` - Pyq
            - model: str. The initial model to use for this prompt <span style="white-space: nowrap">`non-empty`</span>

            - parameters: RegisterPromptModelParametersRequest. The initial model parameters to use for  this prompt

            - meta: typing.Optional[typing.Dict[str, typing.Any]]. Optionally include additional metadata to store along with the prompt.
        """
        _request: typing.Dict[str, typing.Any] = {
            "label": label,
            "name": name,
            "prompt": prompt,
            "provider": provider,
            "model": model,
            "parameters": parameters,
        }
        if meta is not OMIT:
            _request["meta"] = meta
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_environment().default}/", "v1/registered-prompts/register"
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=None,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RegisterPromptResponse, _response.json())  # type: ignore
        if _response.status_code == 409:
            raise ConflictError(pydantic.parse_obj_as(RegisterPromptErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncRegisteredPromptsClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def register_prompt(
        self,
        *,
        label: str,
        name: str,
        prompt: RegisterPromptPromptInfoRequest,
        provider: ProviderEnum,
        model: str,
        parameters: RegisterPromptModelParametersRequest,
        meta: typing.Optional[typing.Dict[str, typing.Any]] = OMIT,
    ) -> RegisterPromptResponse:
        """
        <strong style="background-color:#ffc107; color:white; padding:4px; border-radius:4px">Unstable</strong>

        Registers a prompt within Vellum and creates associated Vellum entities. Intended to be used by integration
        partners, not directly by Vellum users.

        Under the hood, this endpoint creates a new sandbox, a new model version, and a new deployment.

        Parameters:
            - label: str. A human-friendly label for corresponding entities created in Vellum. <span style="white-space: nowrap">`non-empty`</span>

            - name: str. A uniquely-identifying name for corresponding entities created in Vellum. <span style="white-space: nowrap">`non-empty`</span>

            - prompt: RegisterPromptPromptInfoRequest. Information about how to execute the prompt template.

            - provider: ProviderEnum. The initial LLM provider to use for this prompt

                                      * `ANTHROPIC` - Anthropic
                                      * `COHERE` - Cohere
                                      * `GOOGLE` - Google
                                      * `HOSTED` - Hosted
                                      * `MOSAICML` - MosaicML
                                      * `MYSTIC` - Mystic
                                      * `OPENAI` - OpenAI
                                      * `PYQ` - Pyq
            - model: str. The initial model to use for this prompt <span style="white-space: nowrap">`non-empty`</span>

            - parameters: RegisterPromptModelParametersRequest. The initial model parameters to use for  this prompt

            - meta: typing.Optional[typing.Dict[str, typing.Any]]. Optionally include additional metadata to store along with the prompt.
        """
        _request: typing.Dict[str, typing.Any] = {
            "label": label,
            "name": name,
            "prompt": prompt,
            "provider": provider,
            "model": model,
            "parameters": parameters,
        }
        if meta is not OMIT:
            _request["meta"] = meta
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(
                f"{self._client_wrapper.get_environment().default}/", "v1/registered-prompts/register"
            ),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=None,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(RegisterPromptResponse, _response.json())  # type: ignore
        if _response.status_code == 409:
            raise ConflictError(pydantic.parse_obj_as(RegisterPromptErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
